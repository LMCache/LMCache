<!DOCTYPE html>
<html lang="en"
      data-content_root="../../../../"
      x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }"
      x-init="$watch('darkMode', val => localStorage.setItem('darkMode', val))"
      class="scroll-smooth"
      :class="{'dark': darkMode === 'dark' || (darkMode === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)}"
>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta charset="utf-8" />
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="white" />
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="black" />
  
    <title>lmcache.storage_backend.serde.cachegen_encoder | LMCache  documentation</title>
    <meta property="og:title" content="lmcache.storage_backend.serde.cachegen_encoder | LMCache  documentation" />
    <meta name="twitter:title" content="lmcache.storage_backend.serde.cachegen_encoder | LMCache  documentation" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=c5d7c51f" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/theme.css?v=ecdfb4fc" />
      <link rel="icon" href="../../../../_static/lmcache-logo.png" />
        <link rel="search" title="Search" href="../../../../search.html" />
        <link rel="index" title="Index" href="../../../../genindex.html" />

    <script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
</head>
<body x-data="{ showSidebar: false }" class="min-h-screen font-sans antialiased bg-background text-foreground" :class="{ 'overflow-hidden': showSidebar }">
    <div x-cloak x-show="showSidebar" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm md:hidden" @click.self="showSidebar = false"></div><div id="page" class="relative flex flex-col min-h-screen"><a href="#content" class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100">
      Skip to content
    </a><header
  class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
    <div class="hidden mr-4 md:flex">
      <a href="../../../../index.html" class="flex items-center mr-6"><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">LMCache  documentation</span>
      </a></div><button
      class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden"
      type="button" @click="showSidebar = true">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" aria-hidden="true"
        fill="currentColor">
        <path
          d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z" />
      </svg>
      <span class="sr-only">Toggle navigation menu</span>
    </button>
    <div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
      <div class="flex-1 w-full md:w-auto md:flex-none"><form id="searchbox"
      action="../../../../search.html"
      method="get"
      class="relative flex items-center group"
      @keydown.k.window.meta="$refs.search.focus()">
  <input x-ref="search"
          name="q"
          id="search-input"
          type="search"
          aria-label="Search the docs"
          placeholder="Search ..."
          class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" />
  <kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
    <span class="text-xs">âŒ˜</span>
    K
  </kbd>
</form>
      </div>
      <nav class="flex items-center space-x-1">
        <a href="https://github.com/LMCache/LMCache/" title="Visit GitHub" rel="noopener nofollow">
          <div
            class="inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md disabled:opacity-50 disabled:pointer-events-none hover:bg-accent hover:text-accent-foreground h-9 w-9">
            <svg height="26px" style="margin-top:-2px;display:inline" viewBox="0 0 45 44" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M22.477.927C10.485.927.76 10.65.76 22.647c0 9.596 6.223                 17.736 14.853 20.608 1.087.2 1.483-.47 1.483-1.047 0-.516-.019-1.881-.03-3.693-6.04 1.312-7.315-2.912-7.315-2.912-.988-2.51-2.412-3.178-2.412                 -3.178-1.972-1.346.149-1.32.149-1.32 2.18.154 3.327 2.24 3.327 2.24 1.937 3.318 5.084 2.36 6.321 1.803.197-1.403.759-2.36 1.379-2.903-4.823-.548-9.894-2.412-9.894-10.734 0-2.37.847-4.31 2.236-5.828-.224-.55-.969-2.759.214-5.748 0 0 1.822-.584 5.972 2.226 1.732-.482 3.59-.722 5.437-.732 1.845.01 3.703.25 5.437.732 4.147-2.81 5.967-2.226 5.967-2.226 1.185 2.99.44 5.198.217 5.748 1.392 1.517 2.232                  3.457 2.232 5.828 0 8.344-5.078 10.18-9.916 10.717.779.67 1.474 1.996 1.474                 4.021 0 2.904-.027 5.247-.027 5.96 0 .58.392 1.256 1.493 1.044C37.981 40.375 44.2 32.24                  44.2 22.647c0-11.996-9.726-21.72-21.722-21.72" fill="currentColor"/></svg>
          </div>
        </a>
        
        <button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'"
          class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9"
          type="button"
          aria-label="Color theme switcher">
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0">
            <path
              d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z" />
          </svg>
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100">
            <path
              d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z" />
          </svg>
        </button>
      </nav>
    </div>
  </div>
</header>

    <div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside id="left-sidebar"
  class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky"
  :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }">

    <a href="../../../../index.html" class="!justify-start text-sm md:!hidden bg-background"><span class="font-bold text-clip whitespace-nowrap">LMCache  documentation</span>
    </a>

    <div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
      <div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
  <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started/docker.html">LMCache with Docker</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/launching.html">Launching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/backend.html">Selecting a backend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Configuration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../configuration/config.html">Configuring LMCache</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer/lmcache.html">lmcache package</a></li>
</ul>

</nav>
      </div>
    </div>
    <button type="button" @click="showSidebar = false"
      class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
        stroke="none" class="h-4 w-4">
        <path
          d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z" />
      </svg>
    </button>
  </aside>
        <main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs"
     class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
  <a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground"
     href="../../../../index.html">
    <span class="hidden md:inline">LMCache  documentation</span>
    <svg xmlns="http://www.w3.org/2000/svg"
         height="18"
         width="18"
         viewBox="0 96 960 960"
         aria-label="Home"
         fill="currentColor"
         stroke="none"
         class="md:hidden">
      <path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z" />
    </svg>
  </a>
  
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../../../index.html">Module code</a>
    
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../../storage_backend.html">lmcache.storage_backend</a>
    
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../serde.html">lmcache.storage_backend.serde</a>
    
<div class="mr-1">/</div><span aria-current="page"
        class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">lmcache.storage_backend.serde.cachegen_encoder</span>
</nav>

    <div id="content" role="main">
      <h1>Source code for lmcache.storage_backend.serde.cachegen_encoder</h1><div class="highlight"><pre>
<span></span><code><span id="line-1"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
</span><span id="line-2">
</span><span id="line-3"><span class="kn">import</span> <span class="nn">torch</span>
</span><span id="line-4"><span class="kn">import</span> <span class="nn">torchac_cuda</span>  <span class="c1"># type: ignore</span>
</span><span id="line-5">
</span><span id="line-6"><span class="kn">import</span> <span class="nn">lmcache.storage_backend.serde.cachegen_basics</span> <span class="k">as</span> <span class="nn">CGBasics</span>
</span><span id="line-7"><span class="kn">from</span> <span class="nn">lmcache.config</span> <span class="kn">import</span> <span class="n">LMCacheEngineConfig</span><span class="p">,</span> <span class="n">LMCacheEngineMetadata</span>
</span><span id="line-8"><span class="kn">from</span> <span class="nn">lmcache.logging</span> <span class="kn">import</span> <span class="n">init_logger</span>
</span><span id="line-9"><span class="kn">from</span> <span class="nn">lmcache.storage_backend.serde.cachegen_basics</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="line-10">    <span class="n">CacheGenConfig</span><span class="p">,</span> <span class="n">CacheGenGPUBytestream</span><span class="p">,</span> <span class="n">CacheGenGPUEncoderOutput</span><span class="p">)</span>
</span><span id="line-11"><span class="kn">from</span> <span class="nn">lmcache.storage_backend.serde.serde</span> <span class="kn">import</span> <span class="n">Serializer</span>
</span><span id="line-12"><span class="kn">from</span> <span class="nn">lmcache.utils</span> <span class="kn">import</span> <span class="n">_lmcache_nvtx_annotate</span>
</span><span id="line-13">
</span><span id="line-14"><span class="n">logger</span> <span class="o">=</span> <span class="n">init_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="line-15">
</span><span id="line-16">
<div class="viewcode-block" id="torch_quant">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.torch_quant">[docs]</a>
</span><span id="line-17"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-18"><span class="k">def</span> <span class="nf">torch_quant</span><span class="p">(</span><span class="n">bins</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="line-19">                <span class="n">qA</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="line-20"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-21"><span class="sd">    Quantize a float tensor to fixed number of bins</span>
</span><span id="line-22">
</span><span id="line-23"><span class="sd">    Input:</span>
</span><span id="line-24"><span class="sd">        bins: number of bins</span>
</span><span id="line-25"><span class="sd">        qA: the input tensor</span>
</span><span id="line-26">
</span><span id="line-27"><span class="sd">    Returns:</span>
</span><span id="line-28"><span class="sd">        xq: the quantized tensor, in float32</span>
</span><span id="line-29"><span class="sd">        max1: the maximum value of the tensor</span>
</span><span id="line-30"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-31">    <span class="n">MAX</span> <span class="o">=</span> <span class="n">bins</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="line-32">    <span class="n">C</span> <span class="o">=</span> <span class="n">MAX</span>
</span><span id="line-33">    <span class="n">max1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">qA</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="line-34">    <span class="n">xq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">qA</span> <span class="o">*</span> <span class="p">(</span><span class="n">C</span> <span class="o">/</span> <span class="n">max1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
</span><span id="line-35">
</span><span id="line-36">    <span class="k">return</span> <span class="n">xq</span><span class="p">,</span> <span class="n">max1</span></div>

</span><span id="line-37">
</span><span id="line-38">
<div class="viewcode-block" id="torch_quant_vectorized">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.torch_quant_vectorized">[docs]</a>
</span><span id="line-39"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-40"><span class="k">def</span> <span class="nf">torch_quant_vectorized</span><span class="p">(</span>
</span><span id="line-41">        <span class="n">bins</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="line-42">        <span class="n">input_groups</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="line-43"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-44"><span class="sd">    Quantize each group of a tensor to fixed number of bins</span>
</span><span id="line-45">
</span><span id="line-46"><span class="sd">    Input:</span>
</span><span id="line-47"><span class="sd">        bins: number of bins for different layers, with shape [nlayer]</span>
</span><span id="line-48"><span class="sd">        input_groups: with shape [nlayers, ntokens, nchannels]</span>
</span><span id="line-49">
</span><span id="line-50"><span class="sd">    Returns:</span>
</span><span id="line-51"><span class="sd">        quantized groups: [nlayers, ntokens, nchannels]</span>
</span><span id="line-52"><span class="sd">        maxes: [nlayers, ntokens, 1]</span>
</span><span id="line-53"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-54">    <span class="n">MAX</span> <span class="o">=</span> <span class="p">(</span><span class="n">bins</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># shape [nlayers, 1, 1]</span>
</span><span id="line-55">    <span class="n">max1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_groups</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="line-56">                      <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># shape [nlayers, ntokens, 1]</span>
</span><span id="line-57">    <span class="n">factor</span> <span class="o">=</span> <span class="n">MAX</span> <span class="o">/</span> <span class="n">max1</span>  <span class="c1"># shape [nlayers, ntokens, 1]</span>
</span><span id="line-58">    <span class="n">xq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">input_groups</span> <span class="o">*</span> <span class="n">factor</span> <span class="o">+</span> <span class="n">MAX</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="line-59">        <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>  <span class="c1"># shape [nlayers, ntokens, nchannels]</span>
</span><span id="line-60">
</span><span id="line-61">    <span class="k">return</span> <span class="n">xq</span><span class="p">,</span> <span class="n">max1</span></div>

</span><span id="line-62">
</span><span id="line-63">
<div class="viewcode-block" id="concat_max">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.concat_max">[docs]</a>
</span><span id="line-64"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-65"><span class="k">def</span> <span class="nf">concat_max</span><span class="p">(</span><span class="n">max1</span><span class="p">):</span>
</span><span id="line-66"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-67"><span class="sd">    Given a dict of max tensors, concatenate them into a single tensor</span>
</span><span id="line-68"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-69">    <span class="c1"># TODO: this function can be optimized, we don&#39;t really need this</span>
</span><span id="line-70">    <span class="n">maxes</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-71">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">max1</span><span class="p">)):</span>
</span><span id="line-72">        <span class="n">maxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span id="line-73">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">maxes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

</span><span id="line-74">
</span><span id="line-75">
</span><span id="line-76"><span class="k">def</span> <span class="nf">_split_kv</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
</span><span id="line-77"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-78"><span class="sd">    Split a blob KV tensor to K and V tensors with the merged heads</span>
</span><span id="line-79">
</span><span id="line-80"><span class="sd">    Input:</span>
</span><span id="line-81"><span class="sd">        tensor: the KV tensor with shape </span>
</span><span id="line-82"><span class="sd">            [num_layers, 2, num_tokens, num_heads, head_size]</span>
</span><span id="line-83">
</span><span id="line-84"><span class="sd">    Returns:</span>
</span><span id="line-85"><span class="sd">        K and V tensors with shape </span>
</span><span id="line-86"><span class="sd">            [num_layers, num_tokens, num_channels]</span>
</span><span id="line-87"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-88">    <span class="n">num_layers</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="line-89">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span>
</span><span id="line-90">                                       <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">),</span>
</span><span id="line-91">                        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-92">
</span><span id="line-93">
</span><span id="line-94"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-95"><span class="k">def</span> <span class="nf">_convert_to_int_and_normalize</span><span class="p">(</span><span class="n">cdf_float</span><span class="p">,</span> <span class="n">needs_normalization</span><span class="p">):</span>
</span><span id="line-96"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-97"><span class="sd">    Convert floatingpoint CDF to integers. See README for more info.</span>
</span><span id="line-98">
</span><span id="line-99"><span class="sd">    The idea is the following:</span>
</span><span id="line-100"><span class="sd">    When we get the cdf here, it is (assumed to be) between 0 and 1, i.e,</span>
</span><span id="line-101"><span class="sd">      cdf in [0, 1)</span>
</span><span id="line-102"><span class="sd">    (note that 1 should not be included.)</span>
</span><span id="line-103"><span class="sd">    We now want to convert this to int16 but make sure we do not get</span>
</span><span id="line-104"><span class="sd">    the same value twice, as this would break the arithmetic coder</span>
</span><span id="line-105"><span class="sd">    (you need a strictly monotonically increasing function).</span>
</span><span id="line-106"><span class="sd">    So, if needs_normalization==True, we multiply the input CDF</span>
</span><span id="line-107"><span class="sd">    with 2**16 - (Lp - 1). This means that now,</span>
</span><span id="line-108"><span class="sd">      cdf in [0, 2**16 - (Lp - 1)].</span>
</span><span id="line-109"><span class="sd">    Then, in a final step, we add an arange(Lp), which is just a line with</span>
</span><span id="line-110"><span class="sd">    slope one. This ensure that for sure, we will get unique, strictly</span>
</span><span id="line-111"><span class="sd">    monotonically increasing CDFs, which are in [0, 2**16)</span>
</span><span id="line-112"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-113">    <span class="n">PRECISION</span> <span class="o">=</span> <span class="mi">16</span>
</span><span id="line-114">    <span class="n">Lp</span> <span class="o">=</span> <span class="n">cdf_float</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="line-115">    <span class="n">factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span><span id="line-116">                          <span class="n">device</span><span class="o">=</span><span class="n">cdf_float</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">pow_</span><span class="p">(</span><span class="n">PRECISION</span><span class="p">)</span>
</span><span id="line-117">    <span class="n">new_max_value</span> <span class="o">=</span> <span class="n">factor</span>
</span><span id="line-118">    <span class="k">if</span> <span class="n">needs_normalization</span><span class="p">:</span>
</span><span id="line-119">        <span class="n">new_max_value</span> <span class="o">=</span> <span class="n">new_max_value</span> <span class="o">-</span> <span class="p">(</span><span class="n">Lp</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="line-120">    <span class="n">cdf_float</span> <span class="o">=</span> <span class="n">cdf_float</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">new_max_value</span><span class="p">)</span>
</span><span id="line-121">    <span class="n">cdf_float</span> <span class="o">=</span> <span class="n">cdf_float</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
</span><span id="line-122">    <span class="n">cdf</span> <span class="o">=</span> <span class="n">cdf_float</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="line-123">    <span class="k">if</span> <span class="n">needs_normalization</span><span class="p">:</span>
</span><span id="line-124">        <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Lp</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cdf</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-125">        <span class="n">cdf</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
</span><span id="line-126">    <span class="k">return</span> <span class="n">cdf</span>
</span><span id="line-127">
</span><span id="line-128">
<div class="viewcode-block" id="CacheGenEncoderImpl">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenEncoderImpl">[docs]</a>
</span><span id="line-129"><span class="k">class</span> <span class="nc">CacheGenEncoderImpl</span><span class="p">:</span>
</span><span id="line-130">
</span><span id="line-131">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="line-132"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-133"><span class="sd">        Fields:</span>
</span><span id="line-134"><span class="sd">        - fp_kv: </span>
</span><span id="line-135"><span class="sd">            should be a tensor of shape (num_layers, num_tokens, num_channels)</span>
</span><span id="line-136"><span class="sd">        - fp_v: </span>
</span><span id="line-137"><span class="sd">            should be a tensor of shape (num_layers, num_tokens, num_channels)</span>
</span><span id="line-138"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-139">        <span class="bp">self</span><span class="o">.</span><span class="n">fp_k</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fp_k&quot;</span><span class="p">]</span>
</span><span id="line-140">        <span class="bp">self</span><span class="o">.</span><span class="n">fp_v</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fp_v&quot;</span><span class="p">]</span>
</span><span id="line-141">
</span><span id="line-142">        <span class="bp">self</span><span class="o">.</span><span class="n">quantized_key</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="line-143">        <span class="bp">self</span><span class="o">.</span><span class="n">max_tensors_key</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="line-144">        <span class="bp">self</span><span class="o">.</span><span class="n">quantized_value</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="line-145">        <span class="bp">self</span><span class="o">.</span><span class="n">max_tensors_value</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="line-146">        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span>
</span><span id="line-147">
<div class="viewcode-block" id="CacheGenEncoderImpl.quantize">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenEncoderImpl.quantize">[docs]</a>
</span><span id="line-148">    <span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-149">    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="line-150"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Quantize the key and value tensors</span>
</span><span id="line-151"><span class="sd">        (self.fp_k and self.fp_v)</span>
</span><span id="line-152"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-153">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fp_k</span><span class="p">)):</span>
</span><span id="line-154">            <span class="k">if</span> <span class="n">layer</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;key_first_layers&quot;</span><span class="p">]:</span>
</span><span id="line-155">                <span class="n">bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;key_first_bins&quot;</span><span class="p">]</span>
</span><span id="line-156">            <span class="k">elif</span> <span class="n">layer</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;key_second_layers&quot;</span><span class="p">]:</span>
</span><span id="line-157">                <span class="n">bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;key_second_bins&quot;</span><span class="p">]</span>
</span><span id="line-158">            <span class="k">else</span><span class="p">:</span>
</span><span id="line-159">                <span class="n">bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;key_third_bins&quot;</span><span class="p">]</span>
</span><span id="line-160">
</span><span id="line-161">            <span class="n">tmp</span> <span class="o">=</span> <span class="n">torch_quant</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fp_k</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
</span><span id="line-162">            <span class="bp">self</span><span class="o">.</span><span class="n">quantized_key</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">bins</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="line-163">            <span class="bp">self</span><span class="o">.</span><span class="n">max_tensors_key</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="line-164">
</span><span id="line-165">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fp_v</span><span class="p">)):</span>
</span><span id="line-166">            <span class="k">if</span> <span class="n">layer</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;value_first_layers&quot;</span><span class="p">]:</span>
</span><span id="line-167">                <span class="n">bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;value_first_bins&quot;</span><span class="p">]</span>
</span><span id="line-168">            <span class="k">else</span><span class="p">:</span>
</span><span id="line-169">                <span class="n">bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;value_second_bins&quot;</span><span class="p">]</span>
</span><span id="line-170">            <span class="n">tmp</span> <span class="o">=</span> <span class="n">torch_quant</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fp_v</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
</span><span id="line-171">            <span class="bp">self</span><span class="o">.</span><span class="n">quantized_value</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">bins</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="line-172">            <span class="bp">self</span><span class="o">.</span><span class="n">max_tensors_value</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></div>

</span><span id="line-173">
<div class="viewcode-block" id="CacheGenEncoderImpl.compute_cdf">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenEncoderImpl.compute_cdf">[docs]</a>
</span><span id="line-174">    <span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-175">    <span class="k">def</span> <span class="nf">compute_cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_key</span><span class="p">):</span>
</span><span id="line-176"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-177"><span class="sd">        Compute the CDF based on the quantized tensors</span>
</span><span id="line-178"><span class="sd">        Field:</span>
</span><span id="line-179"><span class="sd">        - start_layer: the start layer to compute the CDF</span>
</span><span id="line-180"><span class="sd">        - end_layer: the end layer to compute the CDF</span>
</span><span id="line-181"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-182">        <span class="c1"># TODO: Add start_index here</span>
</span><span id="line-183">        <span class="n">channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fp_k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="line-184">
</span><span id="line-185">        <span class="k">def</span> <span class="nf">process_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">max_val</span><span class="p">):</span>
</span><span id="line-186"><span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-187"><span class="sd">            input shape should be [channels, tokens]</span>
</span><span id="line-188"><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="line-189">            <span class="n">nchannels</span><span class="p">,</span> <span class="n">ntokens</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span><span id="line-190">            <span class="n">one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
</span><span id="line-191">                <span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">max_val</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="line-192">                    <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># Use float32 to avoid integer overflow</span>
</span><span id="line-193">            <span class="n">counts</span> <span class="o">=</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">ntokens</span>
</span><span id="line-194">            <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-195">            <span class="n">ret</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="line-196">            <span class="k">return</span> <span class="n">ret</span>
</span><span id="line-197">
</span><span id="line-198">        <span class="k">def</span> <span class="nf">process_layers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">max_val</span><span class="p">):</span>
</span><span id="line-199"><span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-200"><span class="sd">            x is a iterator of dict values</span>
</span><span id="line-201"><span class="sd">            each element&#39;s shape is [tokens, channels]</span>
</span><span id="line-202"><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="line-203">            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-204">            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
</span><span id="line-205"><span class="w">                </span><span class="sd">&quot;&quot;&quot;do permute here&quot;&quot;&quot;</span>
</span><span id="line-206">                <span class="n">batch_counts</span> <span class="o">=</span> <span class="n">process_batch</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">max_val</span><span class="p">)</span>
</span><span id="line-207">                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_counts</span><span class="p">)</span>
</span><span id="line-208">
</span><span id="line-209">            <span class="n">final_counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="line-210">
</span><span id="line-211">            <span class="k">return</span> <span class="n">final_counts</span>
</span><span id="line-212">
</span><span id="line-213">        <span class="k">if</span> <span class="n">is_key</span><span class="p">:</span>
</span><span id="line-214">            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantized_key</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="line-215">        <span class="k">else</span><span class="p">:</span>
</span><span id="line-216">            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantized_value</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="line-217">        <span class="n">value_range</span> <span class="o">=</span> <span class="mi">32</span>
</span><span id="line-218">        <span class="n">cdfs</span> <span class="o">=</span> <span class="n">process_layers</span><span class="p">(</span>
</span><span id="line-219">            <span class="n">X</span><span class="p">,</span> <span class="n">value_range</span><span class="p">)</span>  <span class="c1"># 4096 is batch size, ==&gt; 18GB GPU memory</span>
</span><span id="line-220">        <span class="n">final_cdf</span> <span class="o">=</span> <span class="n">cdfs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fp_k</span><span class="p">),</span> <span class="n">channels</span><span class="p">,</span> <span class="n">value_range</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="line-221">
</span><span id="line-222">        <span class="k">return</span> <span class="n">final_cdf</span></div>
</div>

</span><span id="line-223">
</span><span id="line-224">
<div class="viewcode-block" id="collect_bytes">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.collect_bytes">[docs]</a>
</span><span id="line-225"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-226"><span class="k">def</span> <span class="nf">collect_bytes</span><span class="p">(</span><span class="n">output_buffer</span><span class="p">,</span> <span class="n">output_lengths</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="line-227"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-228"><span class="sd">    Collect a byte tensor from the output_buffer + output_lengths</span>
</span><span id="line-229"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-230">    <span class="n">output_buffer_size</span> <span class="o">=</span> <span class="n">output_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="line-231">    <span class="n">flattened_lengths</span> <span class="o">=</span> <span class="n">output_lengths</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span id="line-232">    <span class="n">flattened_buffer</span> <span class="o">=</span> <span class="n">output_buffer</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span id="line-233">    <span class="n">summed_length</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_buffer_size</span> <span class="o">-</span> <span class="n">flattened_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="line-234">    <span class="n">summed_length</span> <span class="o">=</span> <span class="n">summed_length</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-235">    <span class="n">summed_length</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="line-236">    <span class="n">indexes</span> <span class="o">=</span> <span class="n">summed_length</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">flattened_lengths</span><span class="p">)</span>
</span><span id="line-237">    <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexes</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">indexes</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-238">    <span class="k">return</span> <span class="n">flattened_buffer</span><span class="p">[</span><span class="n">indexes</span><span class="p">]</span></div>

</span><span id="line-239">
</span><span id="line-240">
<div class="viewcode-block" id="encode_ntokens">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.encode_ntokens">[docs]</a>
</span><span id="line-241"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-242"><span class="k">def</span> <span class="nf">encode_ntokens</span><span class="p">(</span><span class="n">cdf_int</span><span class="p">,</span> <span class="n">encode_input</span><span class="p">,</span> <span class="n">output_buffer</span><span class="p">,</span>
</span><span id="line-243">                   <span class="n">output_lengths</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="line-244"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-245"><span class="sd">    Input:</span>
</span><span id="line-246"><span class="sd">        cdf_int: int16 tensor on GPU with shape [nlayers, nchannels, Lp]</span>
</span><span id="line-247"><span class="sd">        encode_input: int8 tensor on GPU with shape </span>
</span><span id="line-248"><span class="sd">        [nlayers, ntokens, nchannels]</span>
</span><span id="line-249"><span class="sd">        output_buffer: uint8 tensor on GPU with shape </span>
</span><span id="line-250"><span class="sd">        [nlayers, nchannels, BUFFER_SIZE]</span>
</span><span id="line-251"><span class="sd">        output_lengths: int32 tensor on GPU with shape [nlayers, nchannels]</span>
</span><span id="line-252"><span class="sd">    Returns:</span>
</span><span id="line-253"><span class="sd">        byte_tensor: the byte tensor</span>
</span><span id="line-254"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-255">    <span class="n">torchac_cuda</span><span class="o">.</span><span class="n">encode_fast_new</span><span class="p">(</span>
</span><span id="line-256">        <span class="n">cdf_int</span><span class="p">,</span>
</span><span id="line-257">        <span class="n">encode_input</span><span class="p">,</span>
</span><span id="line-258">        <span class="n">output_buffer</span><span class="p">,</span>
</span><span id="line-259">        <span class="n">output_lengths</span><span class="p">,</span>
</span><span id="line-260">    <span class="p">)</span>
</span><span id="line-261">    <span class="n">byte_tensor</span> <span class="o">=</span> <span class="n">collect_bytes</span><span class="p">(</span><span class="n">output_buffer</span><span class="p">,</span> <span class="n">output_lengths</span><span class="p">)</span>
</span><span id="line-262">    <span class="k">return</span> <span class="n">byte_tensor</span></div>

</span><span id="line-263">    <span class="c1"># return byte_tensor.cpu().numpy().tobytes()</span>
</span><span id="line-264">
</span><span id="line-265">
<div class="viewcode-block" id="encode_function">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.encode_function">[docs]</a>
</span><span id="line-266"><span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-267"><span class="k">def</span> <span class="nf">encode_function</span><span class="p">(</span>
</span><span id="line-268">    <span class="n">kv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="line-269">    <span class="n">config</span><span class="p">:</span> <span class="n">CacheGenConfig</span><span class="p">,</span>
</span><span id="line-270">    <span class="n">key_bins</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="line-271">    <span class="n">value_bins</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="line-272">    <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="line-273"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CacheGenGPUEncoderOutput</span><span class="p">:</span>
</span><span id="line-274"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-275"><span class="sd">    Given the path to the original key value cache, encode the KV cache</span>
</span><span id="line-276"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-277">    <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</span><span id="line-278">    <span class="n">fp_k</span><span class="p">,</span> <span class="n">fp_v</span> <span class="o">=</span> <span class="n">_split_kv</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
</span><span id="line-279">    <span class="n">nchannels</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span>
</span><span id="line-280">    <span class="n">nlayers</span> <span class="o">=</span> <span class="n">fp_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">fp_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="line-281">
</span><span id="line-282">    <span class="n">new_key</span><span class="p">,</span> <span class="n">max_tensors_key</span> <span class="o">=</span> <span class="n">torch_quant_vectorized</span><span class="p">(</span><span class="n">key_bins</span><span class="p">,</span> <span class="n">fp_k</span><span class="p">)</span>
</span><span id="line-283">    <span class="n">new_value</span><span class="p">,</span> <span class="n">max_tensors_value</span> <span class="o">=</span> <span class="n">torch_quant_vectorized</span><span class="p">(</span><span class="n">value_bins</span><span class="p">,</span> <span class="n">fp_v</span><span class="p">)</span>
</span><span id="line-284">    <span class="n">encode_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">new_key</span><span class="p">,</span> <span class="n">new_value</span><span class="p">),</span>
</span><span id="line-285">                             <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">nlayers</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">nchannels</span><span class="p">)</span>
</span><span id="line-286">
</span><span id="line-287">    <span class="n">new_cdf_key</span> <span class="o">=</span> <span class="n">torchac_cuda</span><span class="o">.</span><span class="n">calculate_cdf</span><span class="p">(</span><span class="n">new_key</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">key_bins</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
</span><span id="line-288">    <span class="n">new_cdf_value</span> <span class="o">=</span> <span class="n">torchac_cuda</span><span class="o">.</span><span class="n">calculate_cdf</span><span class="p">(</span><span class="n">new_value</span><span class="p">,</span>
</span><span id="line-289">                                               <span class="nb">int</span><span class="p">(</span><span class="n">value_bins</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
</span><span id="line-290">    <span class="n">cdf_int</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">new_cdf_key</span><span class="p">,</span> <span class="n">new_cdf_value</span><span class="p">])</span>
</span><span id="line-291">
</span><span id="line-292">    <span class="n">output_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="line-293">        <span class="p">(</span><span class="n">nlayers</span><span class="p">,</span> <span class="n">nchannels</span><span class="p">,</span> <span class="n">CGBasics</span><span class="o">.</span><span class="n">CACHEGEN_GPU_MAX_TOKENS_PER_CHUNK</span><span class="p">),</span>
</span><span id="line-294">        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
</span><span id="line-295">        <span class="n">device</span><span class="o">=</span><span class="n">encode_input</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
</span><span id="line-296">    <span class="p">)</span>
</span><span id="line-297">    <span class="n">output_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nlayers</span><span class="p">,</span> <span class="n">nchannels</span><span class="p">),</span>
</span><span id="line-298">                                 <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
</span><span id="line-299">                                 <span class="n">device</span><span class="o">=</span><span class="n">encode_input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-300">
</span><span id="line-301">    <span class="n">data_chunks</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-302">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">CGBasics</span><span class="o">.</span><span class="n">CACHEGEN_GPU_MAX_TOKENS_PER_CHUNK</span><span class="p">):</span>
</span><span id="line-303">        <span class="n">start</span> <span class="o">=</span> <span class="n">i</span>
</span><span id="line-304">        <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">CGBasics</span><span class="o">.</span><span class="n">CACHEGEN_GPU_MAX_TOKENS_PER_CHUNK</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
</span><span id="line-305">        <span class="n">bytestream</span> <span class="o">=</span> <span class="n">encode_ntokens</span><span class="p">(</span>
</span><span id="line-306">            <span class="n">cdf_int</span><span class="p">,</span>
</span><span id="line-307">            <span class="n">encode_input</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="p">:],</span>
</span><span id="line-308">            <span class="n">output_buffer</span><span class="p">,</span>
</span><span id="line-309">            <span class="n">output_lengths</span><span class="p">,</span>
</span><span id="line-310">        <span class="p">)</span>
</span><span id="line-311">        <span class="n">data_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="line-312">            <span class="n">CacheGenGPUBytestream</span><span class="p">(</span>
</span><span id="line-313">                <span class="n">bytestream</span><span class="o">=</span><span class="n">bytestream</span><span class="p">,</span>
</span><span id="line-314">                <span class="n">bytestream_lengths</span><span class="o">=</span><span class="n">output_lengths</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
</span><span id="line-315">                <span class="n">ntokens</span><span class="o">=</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span>
</span><span id="line-316">            <span class="p">))</span>
</span><span id="line-317">
</span><span id="line-318">    <span class="k">return</span> <span class="n">CacheGenGPUEncoderOutput</span><span class="p">(</span>
</span><span id="line-319">        <span class="n">data_chunks</span><span class="p">,</span>
</span><span id="line-320">        <span class="n">cdf_int</span><span class="p">,</span>
</span><span id="line-321">        <span class="n">max_tensors_key</span><span class="o">=</span><span class="n">max_tensors_key</span><span class="p">,</span>
</span><span id="line-322">        <span class="n">max_tensors_value</span><span class="o">=</span><span class="n">max_tensors_value</span><span class="p">,</span>
</span><span id="line-323">        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="line-324">        <span class="n">head_size</span><span class="o">=</span><span class="n">head_size</span><span class="p">,</span>
</span><span id="line-325">    <span class="p">)</span></div>

</span><span id="line-326">
</span><span id="line-327">
<div class="viewcode-block" id="CacheGenSerializer">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenSerializer">[docs]</a>
</span><span id="line-328"><span class="k">class</span> <span class="nc">CacheGenSerializer</span><span class="p">(</span><span class="n">Serializer</span><span class="p">):</span>
</span><span id="line-329">
</span><span id="line-330">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LMCacheEngineConfig</span><span class="p">,</span>
</span><span id="line-331">                 <span class="n">metadata</span><span class="p">:</span> <span class="n">LMCacheEngineMetadata</span><span class="p">):</span>
</span><span id="line-332">        <span class="bp">self</span><span class="o">.</span><span class="n">cachegen_config</span> <span class="o">=</span> <span class="n">CacheGenConfig</span><span class="o">.</span><span class="n">from_model_name</span><span class="p">(</span>
</span><span id="line-333">            <span class="n">metadata</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
</span><span id="line-334">        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span>
</span><span id="line-335">        <span class="bp">self</span><span class="o">.</span><span class="n">fmt</span> <span class="o">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span>
</span><span id="line-336">        <span class="bp">self</span><span class="o">.</span><span class="n">key_bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_key_bins</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cachegen_config</span><span class="p">)</span>
</span><span id="line-337">        <span class="bp">self</span><span class="o">.</span><span class="n">value_bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_value_bins</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cachegen_config</span><span class="p">)</span>
</span><span id="line-338">
<div class="viewcode-block" id="CacheGenSerializer.make_key_bins">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenSerializer.make_key_bins">[docs]</a>
</span><span id="line-339">    <span class="k">def</span> <span class="nf">make_key_bins</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CacheGenConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="line-340">        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">key_third_layers</span><span class="p">)</span>
</span><span id="line-341">        <span class="n">ret</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">key_third_bins</span><span class="p">)</span>
</span><span id="line-342">        <span class="n">ret</span><span class="p">[:</span><span class="n">config</span><span class="o">.</span><span class="n">key_second_layers</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">key_second_bins</span>
</span><span id="line-343">        <span class="n">ret</span><span class="p">[:</span><span class="n">config</span><span class="o">.</span><span class="n">key_first_layers</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">key_first_bins</span>
</span><span id="line-344">        <span class="k">return</span> <span class="n">ret</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span></div>

</span><span id="line-345">
<div class="viewcode-block" id="CacheGenSerializer.make_value_bins">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenSerializer.make_value_bins">[docs]</a>
</span><span id="line-346">    <span class="k">def</span> <span class="nf">make_value_bins</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CacheGenConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="line-347">        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">key_third_layers</span><span class="p">)</span>
</span><span id="line-348">        <span class="n">ret</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">value_second_bins</span><span class="p">)</span>
</span><span id="line-349">        <span class="n">ret</span><span class="p">[:</span><span class="n">config</span><span class="o">.</span><span class="n">value_first_layers</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">value_first_bins</span>
</span><span id="line-350">        <span class="k">return</span> <span class="n">ret</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span></div>

</span><span id="line-351">
<div class="viewcode-block" id="CacheGenSerializer.to_bytes">
<a class="viewcode-back" href="../../../../developer/lmcache.storage_backend.serde.html#lmcache.storage_backend.serde.cachegen_encoder.CacheGenSerializer.to_bytes">[docs]</a>
</span><span id="line-352">    <span class="nd">@_lmcache_nvtx_annotate</span>
</span><span id="line-353">    <span class="k">def</span> <span class="nf">to_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bytes</span><span class="p">:</span>
</span><span id="line-354"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-355"><span class="sd">        Serialize a pytorch tensor to bytes. The serialized bytes should contain</span>
</span><span id="line-356"><span class="sd">        both the data and the metadata (shape, dtype, etc.) of the tensor.</span>
</span><span id="line-357">
</span><span id="line-358"><span class="sd">        Input:</span>
</span><span id="line-359"><span class="sd">            t: the input pytorch tensor, can be on any device, in any shape,</span>
</span><span id="line-360"><span class="sd">               with any dtype</span>
</span><span id="line-361">
</span><span id="line-362"><span class="sd">        Returns:</span>
</span><span id="line-363"><span class="sd">            bytes: the serialized bytes</span>
</span><span id="line-364"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-365">        <span class="c1"># Temporary fix for issue #83: encoder will have the default device 0</span>
</span><span id="line-366">        <span class="c1"># on all the ray workers. Need to set it to the correct device.</span>
</span><span id="line-367">        <span class="c1"># Also need to figure out why this happens.</span>
</span><span id="line-368">        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span> <span class="o">!=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
</span><span id="line-369">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-370">        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_bins</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
</span><span id="line-371">            <span class="bp">self</span><span class="o">.</span><span class="n">key_bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_bins</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-372">        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_bins</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
</span><span id="line-373">            <span class="bp">self</span><span class="o">.</span><span class="n">value_bins</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_bins</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-374">
</span><span id="line-375">        <span class="c1"># TODO: permute is expensive here, need a better way to do it at lower</span>
</span><span id="line-376">        <span class="c1"># level</span>
</span><span id="line-377">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fmt</span> <span class="o">==</span> <span class="s2">&quot;huggingface&quot;</span><span class="p">:</span>
</span><span id="line-378">            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span><span id="line-379"><span class="w">        </span><span class="sd">&quot;&quot;&quot; expecting a tensor of shape </span>
</span><span id="line-380"><span class="sd">        [num_layers, 2, num_tokens, num_heads, head_size] &quot;&quot;&quot;</span>
</span><span id="line-381">        <span class="n">ntokens</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span><span id="line-382">        <span class="n">output_dict</span> <span class="o">=</span> <span class="n">encode_function</span><span class="p">(</span>
</span><span id="line-383">            <span class="n">tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
</span><span id="line-384">            <span class="bp">self</span><span class="o">.</span><span class="n">cachegen_config</span><span class="p">,</span>
</span><span id="line-385">            <span class="bp">self</span><span class="o">.</span><span class="n">key_bins</span><span class="p">,</span>
</span><span id="line-386">            <span class="bp">self</span><span class="o">.</span><span class="n">value_bins</span><span class="p">,</span>
</span><span id="line-387">            <span class="n">ntokens</span><span class="p">,</span>
</span><span id="line-388">        <span class="p">)</span>
</span><span id="line-389">        <span class="k">return</span> <span class="n">output_dict</span><span class="o">.</span><span class="n">to_bytes</span><span class="p">()</span></div>
</div>

</span></code></pre></div>
    </div><div class="flex justify-between items-center pt-6 mt-12 border-t border-border gap-4">
</div></div>
        </main>
      </div>
    </div><footer class="py-6 border-t border-border md:py-0">
    <div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
      <div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
        <p class="text-sm leading-loose text-center text-muted-foreground md:text-left">Â© 2024, The LMCache Team&nbsp;Built with <a class="font-medium underline underline-offset-4"
    href="https://www.sphinx-doc.org"
    rel="noreferrer">Sphinx 7.3.7</a></p>
</div>
</div>
</footer>
  </div>
  
    <script src="../../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script defer="defer" src="../../../../_static/theme.js?v=e82a16a3"></script>
  
</body>
</html>